# -*- coding: utf-8 -*-
"""Nasa

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MT1l0-PuXhNN_FHVDD6QuMg-KL45Vlhg
"""

import pandas as pd
import requests
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import KMeans

def load_dataset(url):
    if url.startswith("https://github.com/"):
        # Load from GitHub
        raw_url = url.replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")
        return pd.read_csv(raw_url)
    elif "drive.google.com" in url:
        # Load from Google Drive
        file_id = url.split('/')[-2]
        gdown_url = f"https://drive.google.com/uc?id={file_id}"
        return pd.read_csv(gdown_url)
    elif "huggingface.co" in url:
        # Load from Hugging Face
        dataset_name = url.split('/')[-1]
        dataset = pd.read_csv(f"https://huggingface.co/datasets/{dataset_name}/resolve/main/train.csv")  # Adjust for the correct dataset structure
        return dataset
    else:
        raise ValueError("Unsupported URL. Please provide a valid GitHub, Google Drive, or Hugging Face URL.")

def clean_data(data):
    """
    Cleans the dataset by:
    - Dropping columns where all values are NaN
    - Filling NaN values in numerical columns with the mean
    - Filling NaN values in categorical columns with the mode
    """
    # Step 1: Drop columns that are entirely NaN
    data.dropna(axis=1, how='all', inplace=True)
    print("Dropped columns with all NaN values.")

    # Step 2: Fill NaN values for each remaining column
    for column in data.columns:
        if data[column].isnull().sum() > 0:  # Check if column contains NaN values
            if data[column].dtype in ['float64', 'int64']:  # Numerical column
                mean_value = data[column].mean()
                data[column].fillna(mean_value, inplace=True)
                print(f"Missing values in column '{column}' filled with mean: {mean_value}")
            else:  # Categorical column
                mode_value = data[column].mode()[0]
                data[column].fillna(mode_value, inplace=True)
                print(f"Missing values in column '{column}' filled with mode: {mode_value}")

    print("Data cleaning completed!")
    return data

def check_nan_values(data):
    nan_counts = data.isnull().sum()
    if nan_counts.sum() > 0:
        print("There are still NaN values present in the dataset:")
        print(nan_counts[nan_counts > 0])  # Only show columns with NaNs
    else:
        print("No NaN values remain in the dataset.")

def label_encode(data):
    le = LabelEncoder()
    for column in data.select_dtypes(include=['object']).columns:
        data[column] = le.fit_transform(data[column].astype(str))  # Convert to str to avoid issues
        print(f"Column '{column}' has been label encoded.")
    return data

def perform_kmeans(data, n_clusters):
    # Ensure that the data has no NaN values
    if data.isnull().sum().sum() > 0:
        raise ValueError("Data contains NaN values. Please clean the data before clustering.")

    kmeans = KMeans(n_clusters=n_clusters)
    clusters = kmeans.fit_predict(data)

    # Add cluster labels to the original dataset
    data['Cluster'] = clusters
    return data, kmeans

def visualize_data_grid(clustered_data, kmeans_model, data):
    # Set figure size for better spacing
    fig, axs = plt.subplots(2, 2, figsize=(16, 12))  # 2x2 grid for the 4 visuals

    # 1. Scatter Plot (PCA)
    pca = PCA(n_components=2)
    pca_components = pca.fit_transform(data)
    axs[0, 0].scatter(pca_components[:, 0], pca_components[:, 1], c=clustered_data, cmap='viridis', s=50)
    axs[0, 0].set_title('PCA Scatter Plot (Clusters)', fontsize=14)
    axs[0, 0].set_xlabel('PC1', fontsize=12)
    axs[0, 0].set_ylabel('PC2', fontsize=12)

    # 2. Line Chart (Data Trends)
    axs[0, 1].plot(data)
    axs[0, 1].set_title('Feature Trends Over Data Points', fontsize=14)
    axs[0, 1].set_xlabel('Data Points', fontsize=12)
    axs[0, 1].set_ylabel('Feature Values', fontsize=12)
    axs[0, 1].legend(data.columns, bbox_to_anchor=(1.05, 1), loc='upper left')  # Place legend outside the plot

    # 3. Bar Chart (Cluster Counts)
    unique, counts = np.unique(clustered_data, return_counts=True)
    axs[1, 0].bar(unique, counts, color='skyblue')
    axs[1, 0].set_title('Cluster Counts', fontsize=14)
    axs[1, 0].set_xlabel('Cluster', fontsize=12)
    axs[1, 0].set_ylabel('Number of Points', fontsize=12)

    # 4. Heatmap (Feature Correlations)
    sns.heatmap(data.corr(), ax=axs[1, 1], cmap='coolwarm', annot=True, fmt=".2f", cbar=True)
    axs[1, 1].set_title('Feature Correlation Heatmap', fontsize=14)

    # Adjust layout to avoid overlap
    plt.tight_layout()

    # Adjust spacing between subplots
    plt.subplots_adjust(wspace=0.4, hspace=0.4)

    # Show the plots
    plt.show()

def main():
    # Load dataset
    file_path = input("Enter the path to your CSV file: ")
    data = load_dataset(file_path)

    # Clean dataset
    cleaned_data = clean_dataset(data)

    # Label encode the dataset
    encoded_data = label_encode(cleaned_data)

    # Perform KMeans clustering
    n_clusters = int(input("Enter the number of clusters for KMeans: "))
    clustered_data, kmeans_model = perform_kmeans(encoded_data, n_clusters)

    # Visualize the clusters
    visualize_clusters(clustered_data, kmeans_model)  # Pass both arguments

if __name__ == "__main__":
    main()